{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time \n",
    "\n",
    "torch.manual_seed(21)    #reproducible\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ_input_data.shape (35120, 144, 16, 2)\n",
      "p_ij_rtm.shape (35120, 36)\n",
      "q_ij_rtm.shape (35120, 36)\n",
      "v_m_labels.shape (35120, 144)\n",
      "line_input_data.shape (35120, 36, 2)\n"
     ]
    }
   ],
   "source": [
    "p_ij_rtm = np.load('/home/gmy/workspace/llm-se/V2/data_1-MV-urban--0-sw/use0/p_ij_with_rtm.npy')\n",
    "q_ij_rtm = np.load('/home/gmy/workspace/llm-se/V2/data_1-MV-urban--0-sw/use0/p_ij_with_rtm.npy')\n",
    "PQ_input_data = np.load('/home/gmy/workspace/llm-se/V2/data_1-MV-urban--0-sw/use0/PQ_input_data.npy')\n",
    "v_m_labels = np.load('/home/gmy/workspace/llm-se/V2/data_1-MV-urban--0-sw/use0/v_m_labels.npy')\n",
    "line_input_data = np.stack((p_ij_rtm, q_ij_rtm), axis=-1) \n",
    "print('PQ_input_data.shape', PQ_input_data.shape)\n",
    "print('p_ij_rtm.shape', p_ij_rtm.shape)\n",
    "print('q_ij_rtm.shape', q_ij_rtm.shape)\n",
    "print('v_m_labels.shape', v_m_labels.shape)\n",
    "print('line_input_data.shape', line_input_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "x1 = PQ_input_data  \n",
    "x2 = line_input_data  \n",
    "y1 = v_m_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = (-1, 16, 2)   \n",
    "x1_reshaped = x1.reshape(new_shape)  \n",
    "\n",
    "x2_expanded = np.expand_dims(x2, axis=1)\n",
    "x2_expanded = np.tile(x2_expanded, (1, 144, 1, 1)) \n",
    "\n",
    "print('x2_expanded.shape', x2_expanded.shape)\n",
    "\n",
    "x2_reshaped = x2_expanded.reshape(-1, 36, 2)  \n",
    "\n",
    "y_m_reshaped = y1.reshape(-1,1)\n",
    "\n",
    "print('x1_reshaped.shape', x1_reshaped.shape)\n",
    "print('x2_reshaped.shape', x2_reshaped.shape)\n",
    "print('y_m_reshaped.shape', y_m_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmy/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, EarlyStoppingCallback, get_scheduler\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = Dataset.from_json(\"/data_1-MV-urban--0-sw/gpt2/dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader  \n",
    "\n",
    "def process_func(example):  \n",
    "    MAX_LENGTH = 128  \n",
    "    input_ids = []  \n",
    "    instruction = tokenizer(example[\"instruction\"])  \n",
    "    input_ids = instruction[\"input_ids\"]  \n",
    "    if len(input_ids) > MAX_LENGTH:  \n",
    "        input_ids = input_ids[:MAX_LENGTH]  \n",
    "    return {\"input_ids\": input_ids}  \n",
    "\n",
    "tokenized_ds = ds.map(process_func)  \n",
    "input_ids_subset = tokenized_ds[\"input_ids\"]  \n",
    "\n",
    "# 划分训练集和测试集  \n",
    "train_size = int(x1_reshaped.shape[0] * 0.8)  \n",
    "test_size = x1_reshaped.shape[0] - train_size  \n",
    "\n",
    "class CustomDataset(Dataset):  \n",
    "    def __init__(self, x1, x2, input_ids, y_m, tokenizer, max_length=128):  \n",
    "        self.x1 = torch.FloatTensor(x1)  \n",
    "        self.x2 = torch.FloatTensor(x2)  \n",
    "        self.y_m = torch.FloatTensor(y_m)  \n",
    "        self.max_length = max_length  \n",
    "        \n",
    "        padded_input_ids = []  \n",
    "        for seq in input_ids:  \n",
    "            if len(seq) < max_length:  \n",
    "                padding = [tokenizer.pad_token_id] * (max_length - len(seq))  \n",
    "                padded_seq = list(seq) + padding  \n",
    "            else:  \n",
    "                padded_seq = list(seq[:max_length])  \n",
    "            padded_input_ids.append(padded_seq)  \n",
    "            \n",
    "        self.input_ids = torch.LongTensor(padded_input_ids)  \n",
    "        self.tokenizer = tokenizer  \n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.x1)  \n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        return {  \n",
    "            'x1': self.x1[idx],  \n",
    "            'x2': self.x2[idx],  \n",
    "            'input_ids': self.input_ids[idx],  \n",
    "            'labels': self.y_m[idx]\n",
    "        }  \n",
    "\n",
    "# 训练集  \n",
    "train_dataset = CustomDataset(  \n",
    "    x1[:train_size],  \n",
    "    x2[:train_size],  \n",
    "    input_ids_subset[:train_size],  \n",
    "    y1[:train_size], \n",
    "    tokenizer  \n",
    ")  \n",
    "\n",
    "# 测试集  \n",
    "test_dataset = CustomDataset(  \n",
    "    x1[train_size:],  \n",
    "    x2[train_size:],  \n",
    "    input_ids_subset[train_size:],  \n",
    "    y1[train_size:],  \n",
    "    tokenizer  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "from torch.utils.data import DataLoader  \n",
    "from transformers import AdamW, get_linear_schedule_with_warmup  \n",
    "from tqdm import tqdm  \n",
    "\n",
    "class PatchTSTEncoder(nn.Module):  \n",
    "    def __init__(self, input_dim, hidden_size, patch_len, stride):  \n",
    "        super().__init__()  \n",
    "        self.patch_len = patch_len  \n",
    "        self.stride = stride  \n",
    "        \n",
    "        # Channel-independent patch embedding  \n",
    "        self.patch_embedding = nn.Conv1d(  \n",
    "            in_channels=input_dim,  \n",
    "            out_channels=hidden_size,  \n",
    "            kernel_size=patch_len,  \n",
    "            stride=stride  \n",
    "        )  \n",
    "        \n",
    "        # Position encoding for patches  \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1000, hidden_size))  # Max length of 1000 patches  \n",
    "        \n",
    "        # Transformer layers for patch processing  \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(  \n",
    "            d_model=hidden_size,  \n",
    "            nhead=8,  \n",
    "            dim_feedforward=hidden_size * 4,  \n",
    "            dropout=0.1,  \n",
    "            activation='gelu'  \n",
    "        )  \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)  \n",
    "        \n",
    "    def forward(self, x):  \n",
    "        # x shape: [batch_size, seq_len, input_dim]  \n",
    "        x = x.transpose(1, 2)  # [batch_size, input_dim, seq_len]  \n",
    "        \n",
    "        # Create patches  \n",
    "        patches = self.patch_embedding(x)  # [batch_size, hidden_size, num_patches]  \n",
    "        patches = patches.transpose(1, 2)  # [batch_size, num_patches, hidden_size]  \n",
    "        \n",
    "        # Add position encoding  \n",
    "        num_patches = patches.shape[1]  \n",
    "        patches = patches + self.pos_embedding[:, :num_patches, :]  \n",
    "        \n",
    "        # Apply transformer  \n",
    "        patches = self.transformer_encoder(patches)  \n",
    "        \n",
    "        return patches  \n",
    "\n",
    "class ModifiedGPT2Model(nn.Module):  \n",
    "    def __init__(self, original_model, time_series_dim=2, realtime_dim=2):  \n",
    "        super(ModifiedGPT2Model, self).__init__()  \n",
    "        self.original_model = original_model  \n",
    "        self.hidden_size = original_model.config.hidden_size  \n",
    "        \n",
    "        self.wte = original_model.transformer.wte  \n",
    "        self.wpe = original_model.transformer.wpe  \n",
    "        self.h_blocks = original_model.transformer.h  \n",
    "        self.ln_f = original_model.transformer.ln_f  \n",
    "\n",
    "        # Set parameters frozen/trainable status  \n",
    "        for param in self.wpe.parameters():  \n",
    "            param.requires_grad = True  \n",
    "        for param in self.wte.parameters():  \n",
    "            param.requires_grad = False    \n",
    "        for param in self.h_blocks.parameters():  \n",
    "            param.requires_grad = False   \n",
    "        for param in self.ln_f.parameters():  \n",
    "            param.requires_grad = False  \n",
    "\n",
    "        self.patch_len = 4\n",
    "        self.stride = 2\n",
    "        self.time_series_encoder = PatchTSTEncoder(  \n",
    "            input_dim=time_series_dim,  \n",
    "            hidden_size=self.hidden_size,  \n",
    "            patch_len=self.patch_len,  \n",
    "            stride=self.stride  \n",
    "        )  \n",
    "        \n",
    "        self.realtime_linear = nn.Linear(realtime_dim, self.hidden_size)  \n",
    "        self.realtime_down = nn.Sequential(  \n",
    "            nn.Conv1d(self.hidden_size, self.hidden_size, 3, padding=1),  \n",
    "            nn.ReLU(),  \n",
    "            nn.BatchNorm1d(self.hidden_size)  \n",
    "        )  \n",
    "        \n",
    "        self.alignment_layer = nn.Sequential(  \n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size),  \n",
    "            nn.ReLU(),  \n",
    "            nn.LayerNorm(self.hidden_size)  \n",
    "        )  \n",
    "        \n",
    "        self.regression_head = nn.Sequential(  \n",
    "            nn.Linear(self.hidden_size, 256),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.1),  \n",
    "            nn.Linear(256, 64),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.1),  \n",
    "            nn.Linear(64, 1)  \n",
    "        )  \n",
    "        \n",
    "    def forward(self, input_ids, x1, x2, attention_mask=None):  \n",
    "        inputs_embeds = self.wte(input_ids)  \n",
    "        position_ids = torch.arange(0, input_ids.size(1),  \n",
    "                                  dtype=torch.long, device=input_ids.device)  \n",
    "        position_embeds = self.wpe(position_ids).unsqueeze(0)  \n",
    "        text_hidden = inputs_embeds + position_embeds  \n",
    "\n",
    "        for block in self.h_blocks:  \n",
    "            text_hidden = block(text_hidden)[0]  \n",
    "        text_features = self.ln_f(text_hidden)  # [batch_size, seq_len, hidden_size]  \n",
    "        \n",
    "        time_features = self.time_series_encoder(x1)  # [batch_size, num_patches, hidden_size] #[64, 7, 768]  \n",
    "        realtime_features = self.realtime_linear(x2)  \n",
    "        realtime_features = realtime_features.transpose(1, 2)  \n",
    "        realtime_features = self.realtime_down(realtime_features)  \n",
    "        realtime_features = realtime_features.transpose(1, 2)  \n",
    "\n",
    "        time_features_mean = time_features.mean(dim=1, keepdim=True).expand(-1, text_features.size(1), -1)  \n",
    "        realtime_features_mean = realtime_features.mean(dim=1, keepdim=True).expand(-1, text_features.size(1), -1)  \n",
    "        \n",
    "        combined_features = torch.cat([  \n",
    "            text_features,  \n",
    "            time_features_mean,  \n",
    "            realtime_features_mean  \n",
    "        ], dim=-1)  \n",
    "        \n",
    "        fused_features = self.alignment_layer(combined_features)  \n",
    "        final_features = fused_features[:, -1, :]  \n",
    "        regression_output = self.regression_head(final_features)  \n",
    "        \n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "class Trainer:  \n",
    "    def __init__(self, model, train_dataset, test_dataset, tokenizer, config):  \n",
    "        self.model = model  \n",
    "\n",
    "        #print(model)\n",
    "        print('number of params: %d'%(sum(temp.numel() for temp in model.parameters() if temp.requires_grad)))\n",
    "        self.train_dataset = train_dataset \n",
    "        self.test_dataset = test_dataset  \n",
    "        self.train_loader = DataLoader(  \n",
    "            train_dataset,  \n",
    "            batch_size=config['batch_size'],  \n",
    "            shuffle=True  \n",
    "        )  \n",
    "        self.test_loader = DataLoader(  \n",
    "            test_dataset,  \n",
    "            batch_size=config['batch_size']  \n",
    "        )  \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.config = config  \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "        self.model.to(self.device)  \n",
    "        \n",
    "        self.criterion = nn.MSELoss()  \n",
    "        \n",
    "        self.optimizer = AdamW(  \n",
    "            self.model.parameters(),  \n",
    "            lr=config['learning_rate'],  \n",
    "            weight_decay=config['weight_decay']  \n",
    "        )  \n",
    "        \n",
    "        self.best_val_loss = float('inf')  \n",
    "        self.patience_counter = 0  \n",
    "        self.best_model_state = None  \n",
    "        self.step_counter = 0 \n",
    "        self.early_stop = False\n",
    "        self.training_losses = []  # Will store (step, loss) pairs every 10 steps    \n",
    "\n",
    "    def evaluate(self, sample_ratio=1.0, seed=55):  # Added seed parameter  \n",
    "\n",
    "        self.model.eval()  \n",
    "        total_val_loss = 0  \n",
    "        all_preds = []  \n",
    "        all_labels = []  \n",
    "\n",
    "        import random  \n",
    "        import numpy as np  \n",
    "\n",
    "        random.seed(seed)  \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed)  \n",
    "        if torch.cuda.is_available():  \n",
    "            torch.cuda.manual_seed(seed)  \n",
    "            torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "        if sample_ratio < 1.0:  \n",
    "            subset_size = int(len(self.test_dataset) * sample_ratio)  \n",
    "            generator = torch.Generator()  \n",
    "            generator.manual_seed(seed)  \n",
    "            indices = torch.randperm(len(self.test_dataset), generator=generator)[:subset_size]  \n",
    "            sampler = SubsetRandomSampler(indices, generator=generator)  \n",
    "            temp_loader = DataLoader(  \n",
    "                self.test_dataset,  \n",
    "                batch_size=self.config['batch_size'],  \n",
    "                sampler=sampler,  \n",
    "                worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id) \n",
    "            )  \n",
    "        else:  \n",
    "            temp_loader = self.test_loader  \n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for batch in tqdm(temp_loader, desc='Evaluating'):  \n",
    "                input_ids = batch['input_ids'].to(self.device)  \n",
    "                x1 = batch['x1'].to(self.device)  \n",
    "                x2 = batch['x2'].to(self.device)  \n",
    "                labels = batch['labels'].to(self.device)  \n",
    "\n",
    "                outputs = self.model(input_ids, x1, x2)  \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                total_val_loss += loss.item()  \n",
    "\n",
    "                all_preds.extend(outputs.cpu().numpy())  \n",
    "                all_labels.extend(labels.cpu().numpy())  \n",
    "\n",
    "        avg_val_loss = total_val_loss / len(temp_loader)  \n",
    "        mae = np.mean(np.abs(np.array(all_preds) - np.array(all_labels)))  \n",
    "\n",
    "        return avg_val_loss, mae  \n",
    "    \n",
    "    def train_epoch(self):  \n",
    "        self.model.train()  \n",
    "        total_train_loss = 0  \n",
    "\n",
    "        progress_bar = tqdm(self.train_loader, desc='Training')  \n",
    "        \n",
    "        for batch in progress_bar:  \n",
    "            input_ids = batch['input_ids'].to(self.device)  \n",
    "            x1 = batch['x1'].to(self.device)  \n",
    "            x2 = batch['x2'].to(self.device)  \n",
    "            labels = batch['labels'].to(self.device)  \n",
    "\n",
    "            outputs = self.model(input_ids, x1, x2)  \n",
    "            loss = self.criterion(outputs, labels)  \n",
    "            \n",
    "            self.optimizer.zero_grad()  \n",
    "            loss.backward()  \n",
    "            self.optimizer.step()  \n",
    "            \n",
    "            total_train_loss += loss.item()  \n",
    "            current_loss = loss.item()  \n",
    "            progress_bar.set_postfix({'train_loss': loss.item()})  \n",
    "\n",
    "            self.step_counter += 1  \n",
    "\n",
    "            # Store loss every 10 steps  \n",
    "            if self.step_counter % 1 == 0:  \n",
    "                self.training_losses.append((self.step_counter, current_loss)) \n",
    "                \n",
    "            if self.step_counter % 50 == 0:\n",
    "                val_loss, mae = self.evaluate(sample_ratio=0.1, seed=55)  \n",
    "                print(f\"\\nStep {self.step_counter} - Validation (10% subset):\")  \n",
    "                print(f\"Val Loss: {val_loss:.8f} | MAE: {mae:.8f}\")  \n",
    "                \n",
    "                if val_loss < self.best_val_loss - self.config['min_delta']:  \n",
    "                    self.best_val_loss = val_loss  \n",
    "                    self.patience_counter = 0  \n",
    "                    self.best_model_state = self.model.state_dict()  \n",
    "                    print(f\"Found better model with validation loss: {self.best_val_loss:.8f}\")  \n",
    "                    torch.save(self.best_model_state, self.config['model_save_path'])  \n",
    "                else:  \n",
    "                    self.patience_counter += 1  \n",
    "                    print(f\"Validation loss did not improve. Patience counter: {self.patience_counter}/{self.config['patience']}\")  \n",
    "                \n",
    "                if self.patience_counter >= self.config['patience']:  \n",
    "                    print(f\"\\nEarly stopping triggered at step {self.step_counter}!\")  \n",
    "                    print(f\"Best validation loss achieved: {self.best_val_loss:.8f}\")  \n",
    "                    self.early_stop = True  \n",
    "                    break  \n",
    "\n",
    "        return total_train_loss / len(self.train_loader)  \n",
    "\n",
    "    def train(self):  \n",
    "        print(\"Starting training...\")  \n",
    "        print(f\"Training on device: {self.device}\")  \n",
    "        \n",
    "        for epoch in range(self.config['epochs']):  \n",
    "            avg_train_loss = self.train_epoch()  \n",
    "            \n",
    "            print(f\"\\nEpoch [{epoch+1}/{self.config['epochs']}]\")  \n",
    "            print(f\"Train Loss: {avg_train_loss:.8f}\")  \n",
    "            \n",
    "            if self.early_stop:  \n",
    "                break  \n",
    "        \n",
    "        if self.best_model_state is not None:  \n",
    "            self.model.load_state_dict(self.best_model_state)  \n",
    "            print(\"Loaded best model state from checkpoint\")  \n",
    "            torch.save(self.model.state_dict(), './modified_gpt2_model.pth')  \n",
    "            print(\"Best model saved\")  \n",
    "\n",
    "    def get_training_losses(self):  \n",
    "        return self.training_losses  \n",
    "    \n",
    "def reinitialize_model_parameters(model):  \n",
    "\n",
    "    def _init_weights(module):  \n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):  \n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)  \n",
    "            if isinstance(module, torch.nn.Linear) and module.bias is not None:  \n",
    "                module.bias.data.zero_()  \n",
    "        elif isinstance(module, torch.nn.LayerNorm):  \n",
    "            module.bias.data.zero_()  \n",
    "            module.weight.data.fill_(1.0)  \n",
    "            \n",
    "    # 应用初始化  \n",
    "    model.apply(_init_weights)  \n",
    "    \n",
    "    if hasattr(model, 'wpe'):  \n",
    "        model.wpe.weight.data.normal_(mean=0.0, std=0.02)  \n",
    "    \n",
    "    print(\"reinitialize succeed\")  \n",
    "    \n",
    "    return model  \n",
    "\n",
    "# 使用示例：  \n",
    "config = {  \n",
    "    'batch_size': 64,  \n",
    "    'learning_rate': 2e-5,  \n",
    "    'weight_decay': 0.01,  \n",
    "    'epochs': 300,  \n",
    "    'max_grad_norm': 1.0,  \n",
    "    'model_save_path': '/home/gmy/workspace/llm-se/V2/gpt2/checkpoints/best_model_mm.pt',  \n",
    "    'patience': 5,  \n",
    "    'min_delta': 0  \n",
    "}  \n",
    "\n",
    "from transformers import AutoModelForCausalLM  \n",
    "original_model = AutoModelForCausalLM.from_pretrained('/home/gmy/workspace/LLMs_local_dir/gpt2')  \n",
    "model = ModifiedGPT2Model(original_model)  \n",
    "#model = reinitialize_model_parameters(model)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tokenizer, config)  \n",
    "\n",
    "trainer.train()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
